#!/usr/bin/env python3


########################### CHANGELOG #######################
# The purpose of this module is to create a set of functions and one primary class
# that will be useful for the analysis of fiber photometry data generated by 
# the Doric Fiber Photometry system. 
# At this point, the document has already gone through quite a bit of refining. 
# However, as of today I intend to add a function that will allow the class object to 
# be generated by reading in an existing pickled version of the class object, so that 
# updates can be applied to said version without having to read in the raw data file. 
# Obviously, without version control, attempting to do this would be an absolute nightmare. 

# So, moving forward, anything prior to this moment will be considered 1.0.0
# Changes will be recorded here, the version number will be updated and incorporated in any class objects created. 

# May 19, 2022: Adding ability to read in pickled signal_processing_objects for easier integration of future updates.
#               Version tracking has also been added to signal_processing_object.__init__ using the global variable
#               __version__ defined at the end of the changelog. ALWAYS UPDATE __version__ AFTER RECORDING A CHANGE.
#               1.1.0
# May 19, 2022: Debugged shuffle_align. There were some unused variables that were holdovers from testing the logic out in a notebook.
#               1.1.1
__version__='1.1.1'



import pandas as pd
import numpy as np
import warnings
import seaborn as sb
from scipy import signal
import random
import pickle as pkl


###
# AUXILIARY FUNCTIONS
###

def butter_lowpass(cutoff, nyq_freq, order=4):
    if float(cutoff)/nyq_freq >= 1.0:
        warnings.warn(f'Target cutoff frequency ({cutoff}) for isosbestic signal filter exceeds signal nyquist frequency ({nyq_freq}).\n\
                        Cutoff frequency will be set to nyquist frequency - 1 {nyq_freq-1}.')
        cutoff = nyq_freq - 1


    normal_cutoff = float(cutoff) / nyq_freq
    b, a = signal.butter(order, normal_cutoff, btype='lowpass')
    return b, a

def butter_lowpass_filter(data, cutoff_freq, nyq_freq, order=4):
    # Source: https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform
    b, a = butter_lowpass(cutoff_freq, nyq_freq, order=order)
    y = signal.filtfilt(b, a, data)
    return y

def trap_sum(y, x):
    firstPointIDX = 0
    secondPointIDX = 1
    AUC = 0

    while secondPointIDX < len(y):
        rectArea = y[firstPointIDX] * abs(x[secondPointIDX]- x[firstPointIDX])
        triArea = 0.5 * abs(x[secondPointIDX]- x[firstPointIDX]) * abs(y[secondPointIDX]- y[firstPointIDX])
        AUC += rectArea + triArea
        firstPointIDX +=1
        secondPointIDX +=1

    return AUC

def bidirectional_trap_sum(y, x):
    '''
        Calculate the area under the curve with respect to 0, counting sections below zero as negative
        towards the final output. Accomplishes this by inserting 0s at all transition points, and calculating
        the AUC between each set of y=0 pairs. 

        param y: list-like containing y-values.
        param x: list-like containing x-values.
        return:  Area under the curve (as float) according the stated calculation above. 
    '''


    # Sanitize input type to array, and create copies.
    yArray = np.array(y)
    xArray = np.array(x, dtype='float64')

    # Identify points where signal crosses zero (np.diff(yArray>=0))
        # FALSE - FALSE = FALSE
        # TRUE - TRUE = FALSE
        # TRUE - FALSE = TRUE
        # FALSE - TRUE = TRUE
    
    # Use the indexer created by the above logic to get a list of indices at which signal
    # changes sign.
    inflectionPoints = np.where(np.diff(yArray>=0))[0]


    # Store inflection points as coordinate pairs on either side of 
    # each transition.
    transitionSections = {'x': [], 'y': []}
    inflectionsAtZeros = []
    for i in inflectionPoints:
        # If 0 is already a value in the signal, nothing need be done with this point.
        if 0 in y[i:i+2]:
            inflectionsAtZeros.append(i)
        # Otherwise, grab the line segment that crosses 0
        else:
            transitionSections['y'].append(yArray[i:i+2])
            transitionSections['x'].append(xArray[i:i+2])

    # Don't insert 0s where they already exist. Delete these indices from the iterable.

    indices_of_inflections_to_delete = []
    for inflection in inflectionsAtZeros:
        indices_of_inflections_to_delete.extend(np.where(inflectionPoints==inflection)[0])

    inflectionPoints = np.delete(inflectionPoints, indices_of_inflections_to_delete)

    # Calculate x values at which to y=0
    inflectionXs = []
    for i in range(len(inflectionPoints)):
        y1, y2 = transitionSections['y'][i]
        x1, x2 = transitionSections['x'][i]
        
        # Calculate the portion of the section between y1 and 0, and then
        # Multiply the total run by the same portion to determine how much to add
        # to x1 to get the target value. 
        target_x = x1 + (((0-y1)/(y2-y1))*(x2-x1))

        inflectionXs.append(target_x)

    
    # Insert 0s and corresponding x values into xArray and yArray.
    xArray = np.insert(xArray, inflectionPoints+1, np.array(inflectionXs))
    yArray = np.insert(yArray, inflectionPoints+1, np.zeros([1, inflectionPoints.size]))



    #  With 0s added at all transition points, identify these points.
    zeroBorders = np.where(yArray==0)[0].astype('int')

    # If the signal happens to start at 0, omit this point. 
    if np.any(zeroBorders==0):
        zeroBorders = zeroBorders[1:]

    # Pair up identified transition points to create sections for AUC calculation
    if len(zeroBorders)>0:
        signalWindows = [(0, zeroBorders[0])]
        signalWindows.extend([(start, stop) for start, stop in zip(zeroBorders[:-1], zeroBorders[1:])])
        signalWindows.append((zeroBorders[-1], yArray.size))       
    # ...unless the entirety of the signal is already all on one side of 0.
    else:
        signalWindows = [(0, yArray.size)]

    # Iterate over identified windows and calculate AUC for each one. 
    windowAUCs = []
    windowSigns = []
    for borders in signalWindows:
        signalSlice = yArray[borders[0]:borders[1]]
        xValSlice = xArray[borders[0]:borders[1]]

        # AUC is always positive because it is calculated on the absolute signal.
        windowAUCs.append(trap_sum(abs(signalSlice), xValSlice))

        # Determine whether the AUC should be considered positive or negative based on max value.
        if signalSlice.max() > 0:
            windowSigns.append(1)
        else:
            windowSigns.append(-1)


    # "Sign" and sum all AUCs to calculate final value.
    return sum(np.array(windowAUCs)*np.array(windowSigns))

def calc_robust_z(input_signal):
    '''
        Calculates a Robust Median Z scaled to the standard normal distribution
        param input_signal:       Raw input signal to transform
        return normalized_signal: Median z-scaled signal
    '''

    median = np.median(input_signal)
    MAD = np.median(abs(input_signal-median)) * 1.4826
    normalized_signal = (input_signal - median) / MAD   

    return normalized_signal



# SIGNAL PROCESSING OBJECT CLASS

# TODO: Comment __init__ function and class more generally. 
class sig_processing_object(object):
    def __init__(self, input_file, remove_artifacts=True, from_pickle=False):
        # INITIATE FROM PICKLED SPO
        if from_pickle:
            # Load the file:
            with open(input_file, 'rb') as in_file:
                input_spo = pkl.load(in_file)

            # VERSION CHECKING
            try: 
                major_version, minor_version, patch_version = input_spo.__version__.split('.')
            except AttributeError:
                # If the spo was created before version control was implemented, it is from v1.1.0
                major_version, minor_version, patch_version = ('1', '0', '0')
                input_spo.__version__ = '1.0.0' # This is just to make referring to this easier down the line. 

            if major_version != __version__.split('.')[0]:
                # Force exit if there's likely to be issues with backwards compatability. 
                raise Exception(f'Version of input_file {input_file.__version__} is not supported. \
                                  Current Version of Doric Processing Toolbox is {__version__}.')

            elif input_spo.__version__ != __version__:
                # If there aren't likely to be any issues, at least let your user know if there's a difference. 
                warnings.warn(f'Reading in an old signal_processing_object: v{input_spo.__version__}. Current version is v{__version__}.')
            
            # ACTUAL IMPORT

            # The easiest way to do this is just to iterate over the existing attributes within the old spo and store them in the new one. 
            for attribute_name, attribute_value in input_spo.__dict__.items():
                self.__dict__[attribute_name] = attribute_value


        # INITIATE FROM RAW DATA
        else:
            self.input_data_frame = pd.read_csv(input_file, skiprows=1)
            
            # Purge extraneous columns
            # AOut-1/2 are just the sin functions corresponding to the LED driver.
            for col in ['AOut-1', 'AOut-2']:
                if col in self.input_data_frame.columns:
                    self.input_data_frame.drop(col, axis=1, inplace=True)
            # Occasionally, empty columns will be picked up. They are named "Unnamed: X", where X is the column number
            unnamed_col_locs = ['Unnamed' in i for i in self.input_data_frame.columns]
            unnamed_col_ids = self.input_data_frame.columns[unnamed_col_locs]
            self.input_data_frame.drop(unnamed_col_ids, axis=1, inplace=True)

            # Now rename columns to something more recognizable.
            input_file_col_names = {'Time(s)': 'Time', 
                                    'AIn-1 - Dem (AOut-1)': 'Signal',
                                    'AIn-1 - Dem (AOut-2)': 'Isosbestic',
                                    'AIn-1 - Raw': 'TotalRaw',
                                    'DI/O-1': 'TTLOn'}
            try:
                new_col_names = [input_file_col_names[n] for n in self.input_data_frame.columns]
            except KeyError:
                print('Not all column names were found in renaming dictionary.')
                new_col_names = []
                # If the columns can't be renamed in one go, then you'll just have to do it iteratively.
                for old_col_name in self.input_data_frame.columns:
                    # If the column name is one of the ones you expected, change it to the standard.
                    if old_col_name in input_file_col_names.keys():
                        new_col_names.append(input_file_col_names[old_col_name])
                    # Otherwise, just leave it untouched, and let the user know.
                    else:
                        print(f'{old_col_name} not found in input_file_col_names.')
                        new_col_names.append(old_col_name)

            for old_name, new_name in zip(self.input_data_frame.columns, new_col_names):
                print(f'{old_name}-->{new_name}')   
            self.input_data_frame.columns = new_col_names

            # Remove any rows with missing data.
            self.input_data_frame.dropna(how='any', inplace=True)

            # Set up attributes.
            self.sampling_rate = 1/np.diff(self.input_data_frame.Time).mean()
            self.signal_processing_log = []
            self.timestamps = self.input_data_frame.Time.values
            self.signal = self.input_data_frame.Signal.values
            self.isosbestic = self.input_data_frame.Isosbestic.values

            # The doric system introduces a strange artifact every ~937 seconds in which the signal on both channels cuts out. 
            # Remove these artifacts. 
            if remove_artifacts:
                self.remove_artifacts(reference_channel='Isosbestic', threshold=-10)
                self.remove_artifacts(reference_channel='Signal', threshold=-15)
                # Occasionally, I will note that a clear artifact occurs only in the signal channel. As such, filtering should occur using
                # both channels as the references, but we will be more stringent using the Signal channel as the reference. 

            # To avoid memory issues, it helps to delete the input data frame

            # However, I prefer the greater temporal specificity offered by the raw input for calculating TTL times. 
            # As such, try to run identify_TTLs first. 
            try:
                self.identify_TTLs()
            except AttributeError:
                # If there are no TTLs, TTLOn won't exist and it will throw an attribute error. In this case, skip it. 
                pass
            del self.input_data_frame

            # Apply lowpass butterworth filter to isosbestic channel.   
            self.isosbestic = butter_lowpass_filter(self.isosbestic, 40, self.sampling_rate/2, order=4)
            self.signal_processing_log.append(f'Butterworth lowpass filter (40Hz) applied to isosbestic channel.')
            # By performing this step here, you can apply whatever downsample you want to the processed data, because 
            # the isosbestic will not have to be refiltered.  
           
        # Track the version number.  
        self.__version__ = __version__

    def downsample_signal(self, downsample_factor):
        
        #Prior to downsampling, determine target sampling rate and apply anti-aliasing filter.
        target_sampling_rate = self.sampling_rate / downsample_factor
        new_nyq = target_sampling_rate/2

        # Apply filter to signal and isosbestic to remove all frequencies above the nyquist frequency of the downsampled sampling rate.
        self.signal = butter_lowpass_filter(self.signal, new_nyq, self.sampling_rate/2)
        self.isosbestic = butter_lowpass_filter(self.isosbestic, new_nyq, self.sampling_rate/2)
        
        self.signal_processing_log.append(f'{new_nyq}Hz anti-aliasing filter applied to Signal and Isosbestic.')
        
        downsampler_index = np.arange(0, self.signal.size, downsample_factor)

        self.timestamps = self.timestamps[downsampler_index]
        self.signal = self.signal[downsampler_index]
        self.isosbestic = self.isosbestic[downsampler_index]


        if 'deltaF/F calculated.' in self.signal_processing_log:
            warnings.warn(f'Input data downsampled. Re-calculate DFF.')
            try:
                del self.processed_dataframe
                self.signal_processing_log.append('self.processed_dataframe removed following downsampling.')
            except AttributeError:
                pass



        self.signal_processing_log.append(f'Signal downsampled by a factor of {downsample_factor}. {self.sampling_rate}-->{target_sampling_rate}')
        self.sampling_rate = target_sampling_rate
        return self.signal_processing_log



    # Fit the processed isosbestic signal was fitted to the excitation signal using a linear fit to correct for signal decay. 
    def calc_dff(self):
        '''
            A general function for calculating the DeltaF/F. 

            This function filters the isosbestic channel, fits it to the signal channel, and then calculates dF/F from the fit.
                param   self:                       current attributes of the signal_processing_object
                create  self.fitted_isosbestic:     array containing filtered isosbestic with linear fit applied
                create  self.dff:                   array containing excitation signal normalized relative to the filtered and fitted isosbestic 
                return  self.signal_processing_log: list containing a record of processing steps so far applied to the data.
        '''

        # Fit isosbestic channel to signal channel using linear fit.
        fit_coefs = np.polyfit(self.isosbestic, self.signal, 1)
        self.fitted_isobestic = (fit_coefs[0] * self.signal) + fit_coefs[1]

        self.signal_processing_log.append('Linear fit applied to isosbestic channel.')

        # Calculate the DeltaF / F. 
        self.dff = (self.signal - self.fitted_isobestic) / self.fitted_isobestic
        self.signal_processing_log.append(r'deltaF/F calculated.')

        return self.signal_processing_log

        
    def z_norm_deltaff(self, ref_start_time = 0, ref_end_time = 'end'):
        '''
            Convert DeltaF/F to Robust Z scores. 
            param  self:                       attributes of signal_processing_object
            create self.normalized_signal:     robust z normalized DeltaF/F
            return self.signal_processing_log: list containing a record of processing steps so far applied to the data.
        '''

        try:
            if ref_start_time == 0:
                start_idx = 0
            else:
                # Count the number of timestamps below the given start time. 
                start_idx = sum(self.timestamps<=ref_start_time)
           
            if ref_end_time == 'end':
                end_idx = self.dff.size-1
            else:
                # Count the number of timestamps below the given end time. 
                end_idx = sum(self.timestamps<=ref_end_time)
            

            med = np.median(self.dff[start_idx:end_idx])
            MAD = np.median(abs(self.dff[start_idx:end_idx]-med))*1.4826
            # By multiplying MAD by 1.4826 (equivalent to multiplying the numerator by 0.6745), the resultant values
            # will scale with "normal" z-scores. So, a z-score of ~2 will once again correspond to alpha=0.05. 
            self.normalized_signal = (self.dff-med)/MAD
            self.signal_processing_log.append(r'Robust Z-Score normalization performed on deltaF/F.')

            return self.signal_processing_log
        except AttributeError as e:
            raise Exception('Cannot normalize deltaF/F if deltaF/F has not been created! Run calc_dff().') from e


    def remove_artifacts(self, reference_channel = 'Isosbestic', threshold = -10, buffer_size = 50):
        '''
            The artifacts present in data from the Doric system are strange and appear to be the product of the signal cutting out
            approximately once every 937 seconds. The best way to filter out crud is to operate directly on the signal and isosbestic.

            param self:                        attributes of sig_processing_object
            param reference_channel:           The channel (isosbestic or signal) to identify outliers in. 
            param threshold:                   The Z score threshold used to identify drops in signal
            buffer_size:                       The number of frames on either side of the identified artifact window to include in the smooth.
            return self.signal_processing_log: list containing a record of processing steps so far applied to the data.
        '''
        
        # Normalize the isosbestic signal using robust median z method
        if reference_channel == 'Isosbestic':
            sig_channel = self.isosbestic
        elif reference_channel == 'Signal':
            sig_channel = self.signal
        else:
            print(f'Method remove_artifacts does not recognize reference_channel "{reference_channel}". Channel must be Signal or Isosbestic. Defaulting to Isosbestic.')
            sig_channel = self.isosbestic

    
        med = np.median(sig_channel)
        mad = np.median(abs(sig_channel-med))*1.4826
        robust_z_trace = (sig_channel-med)/mad

        # Identify points at which robust_z_trace crosses threshold. 
        switch_points = np.diff(robust_z_trace<threshold, prepend=0)
        # Switch point has 3 possible values: 
        #   1: indicates that signal dropped below threshold (True - False)
        #  -1: indicates that signal came above threshold (False - True)
        #   0: Indicates that signal remained on one side of threshold (True - True | False - False)

        # Identify the points at which an artifact begins and ends. Step buffer_size frames to either side of the 
        #   threshold defined artifact window to ensure that it is entirely excised.
        art_start = np.where(switch_points==1)[0]-buffer_size
        art_end = np.where(switch_points==-1)[0]+buffer_size
        # N.B. The size of this step relative to the signal changes depending on the sampling rate of the file. 
        # With the default sampling rate of 12k Hz, the default buffer_size of 50 frames should be about 4.1667 ms. 


        # Directly remove artifacts from signal and isosbestic
        for start, end in zip(art_start, art_end):
            
            # The artifact period will be overwritten with values interpolated between two values on either side
            #   of the artifact. 

            # The first step in this process is to identify values on either side of the artifact: 

            # Time is common between signal and isosbestic:
            time_firstpoint, time_endpoint = (self.timestamps[start], self.timestamps[end])

            # Identify signal level independently for excitation signal and isosbestic:
            sig_fp, sig_ep = (self.signal[start], self.signal[end])
            iso_fp, iso_ep = (self.isosbestic[start], self.isosbestic[end])

            # Calculate the interpolated values ...
            timepoints_for_interp = self.timestamps[start:end]

            sig_interp = np.interp(x = timepoints_for_interp, 
                                   xp = [time_firstpoint, time_endpoint], 
                                   fp = [sig_fp, sig_ep])
            
            iso_interp = np.interp(x = timepoints_for_interp, 
                                   xp = [time_firstpoint, time_endpoint], 
                                   fp = [iso_fp, iso_ep])

            
            # and insert them into each
            self.signal[start:end] = sig_interp
            self.isosbestic[start:end] = iso_interp


        # Record the process
        self.signal_processing_log.append(f'Artifacts removed from Signal and Isosbestic using threshold={threshold} on {reference_channel}.')
        return self.signal_processing_log

    def create_dataframe(self):
        '''
            Combine current timestamps, DeltaF/F, and normalized DeltaF/F into DataFrame.
            param  self:                       attributes of signal_processing_object
            create self.processed_dataframe:   DataFrame with index = Timestamps, and Columns = raw and normalized DeltaF/F signal
            return self.signal_processing_log: list containing a record of processing steps so far applied to the data.
        '''
        try:
            self.processed_dataframe = pd.DataFrame(index=self.timestamps, columns = ['RawSignal', 'Z_Signal'], 
                                                    data=np.hstack([self.dff.reshape(-1, 1), self.normalized_signal.reshape(-1, 1)]))
            self.signal_processing_log.append('Timestamps, Raw Signal, and Normalized Signal combined into DataFrame (self.processed_dataframe).')
            return self.signal_processing_log
        except AttributeError as e:
            raise Exception('Missing attributes. Run calc_dff() and z_norm_deltaff().') from e

    def trim_signal(self, start_time, end_time):
        '''
            Trims self.signal, self.isosbestic, and self.timestamps.
            param self:                       attributes of signal_processing_object
            param start_time:                 minimum start time to keep after trimming, inclusive
            param stop_time:                  maximum start time to keep after trimming, inclusive          
            return self.signal_processing_log: list containing a record of processing steps so far applied to the data.

            usage: self.trim_signal(50, 100) to trim the signals from 50 to 100 seconds
                   self.trim_signal(100, 'end') to trim signals from 100 seconds to the end.
        '''
        if end_time == 'end':
            end_time = self.timestamps.max()

        trimmed_indices = np.where((self.timestamps>=start_time)&(self.timestamps<=end_time))
        self.timestamps = self.timestamps[trimmed_indices]
        self.signal = self.signal[trimmed_indices]
        self.isosbestic = self.isosbestic[trimmed_indices]

        self.signal_processing_log.append(f'Signal cropped to {start_time}s:{end_time}s.')
        
        if 'deltaF/F calculated.' in self.signal_processing_log:
            warnings.warn('Raw signal has been trimmed. Re-run calc_dff() and z_norm_delta() to perform normalizations on trimmed data.')

            try:
                del self.processed_dataframe
                self.signal_processing_log.append('self.processed_dataframe removed following signal trimming.')
            except AttributeError:
                pass

        return self.signal_processing_log


    def identify_peaks_GunaydinMethod(self):
        '''
            A heuristic for identifying peaks in signal. Taken from https://dx.doi.org/10.1016%2Fj.cell.2014.05.017
            param  self:                       attributes of signal_processing_object
            create self.peak_timestamps:       timestamps at which a peak has been identified
            return self.peak_timestamps:       ''

        '''

        #1) take the derivative of the squared difference between two different low pass filtered copies of the time series data (at 40Hz and 0.4Hz);
        
        # Generate low pass filtered copies at 40Hz and 0.4Hz. 
        try:
            filt_40hz = butter_lowpass_filter(self.dff, 40, self.sampling_rate/2)
            filt_point4hz = butter_lowpass_filter(self.dff, 0.4, self.sampling_rate/2)
        except ValueError as e:
            raise Exception(f'Data must have sampling rate of >80Hz to apply Gunyadin method. Signal has sampling rate {self.sampling_rate}.') from e

        # Calculate the squared difference between the copies. 
        sq_diff = (filt_40hz - filt_point4hz)**2
        

        #2) threshold these values at 95% confidence interval (Z=2)
        
        # Normalize the signal for comparing to confidence interval threshold
        sq_diff_med = np.median(sq_diff)
        sq_diff_MAD = np.median(abs(sq_diff-sq_diff_med))*1.4826
        sq_diff_normalized = (sq_diff-sq_diff_med)/sq_diff_MAD

        # Calculate the derivative
        dy = np.diff(sq_diff_normalized, prepend=0)
        dx = 1/self.sampling_rate
        deriv = dy/dx

        # Identify points where derivative exceeds threshold
        deriv_peaks = np.where(deriv>=2)



        #3) classify these smooth derivative-based time points as ‘‘peaks’’ if absolute fluorescence measurement is above 95% confidence interval
        robust_z_peaks = np.where(abs(self.normalized_signal)>=2)
        self.peak_timestamps = np.intersect1d(self.timestamps[deriv_peaks], self.timestamps[robust_z_peaks])
        

        self.signal_processing_log.append('Fluorescent peaks (and/or troughs) identified according to Gunyadin et al., 2014.')

    # TODO Write peak detection algorithm according to https://doi.org/10.1126/science.aat8078



    # TODO Comment these.
    def identify_TTLs(self):
      switch_points = np.diff(self.input_data_frame.TTLOn, prepend=0)

      ttl_starts = np.where(switch_points==1)
      self.ttl_starts = self.input_data_frame.loc[ttl_starts, 'Time'].values

      self.signal_processing_log.append('TTL onset timestamps identified.')


    def align_to_TTLs(self, baseline_time = 10, epoch_time = 10, signal_to_slice='Z_Signal'):
        # Determine number of bins to devote to baseline.
        n_baseline_bins = int(baseline_time * self.sampling_rate)
        # The next frame after n_baseline_bins is where we want to ensure that our 0 ends up in all slices. 
        target_zero_index = n_baseline_bins+1

        slices = []
        for time in self.ttl_starts:
            # Take an initial slice of timestamps based on baseline and epoch lengths relative to ttl_start 
            timestamps_bools = (self.processed_dataframe.index>(time-baseline_time)) & (self.processed_dataframe.index<(time+epoch_time))
            timestamps = self.processed_dataframe.index[timestamps_bools]


            isi = 1 / self.sampling_rate
            real_sample_spacing = np.diff(timestamps)
            real_sample_spacing = np.insert(real_sample_spacing, 0, isi)

            off_indices = np.where(real_sample_spacing>=isi*1.05)[0]

            padded_signal = self.processed_dataframe.loc[timestamps, signal_to_slice].values

            for diff_idx in np.flip(off_indices):
                n_samples_skipped = np.round(real_sample_spacing[diff_idx] / isi)-1

                for i in np.arange(n_samples_skipped):
                    padded_signal = np.insert(padded_signal, diff_idx, np.nan)

            


            # Get indices of timestamps that currently start and end the slice window.
            slice_start_idx = np.where(self.processed_dataframe.index==timestamps[0])[0][0] # np.where returns an array within a tuple. [0][0] gets the value.
            slice_end_idx = np.where(self.processed_dataframe.index==timestamps[-1])[0][0]


            # In all likelihood, these timestamps will be a little offset from what we want. 
            # Calculate the current offset. 

            # How do we get the current zero index when we're working from the padded signal? We can start off by getting signal closest to TTL: 
            time_closest_to_zero = timestamps[np.argmin(abs(timestamps - time))]
            sig_at_zero = self.processed_dataframe.loc[time_closest_to_zero, signal_to_slice]

            # Now we know the value of the signal at 0. 

            # We don't know for sure that 0 is the only time that the signal is at this value. So, we grab all instances, and their timestamps
            zero_candidate_indices = np.where(padded_signal==sig_at_zero)[0]
            zero_candidates_timestamps = self.processed_dataframe[self.processed_dataframe[signal_to_slice]==sig_at_zero].index
            

            current_zero_index = zero_candidate_indices[np.where(zero_candidates_timestamps==time_closest_to_zero)[0]][0]            

            zero_offset = target_zero_index - current_zero_index # Distance of the above from where we want it. 
                # The sign of zero_offset will indicate the necessary direction of the frame shift. 
                # The absolute magnitude of zero_offset indicates the number of frames to shift by. 

            # If target_zero_index is LESS than current_zero_index, a number of elements must be removed from the baseline to shift 
                # the window to the right.  
            if zero_offset < 0:
                padded_signal = np.delete(padded_signal, range(0, abs(zero_offset)))
            # If target_zero_index is greater than the current_zero_index, then the current window must be shifted to the left
                # That is, a number of elements must be added to baseline 
            elif zero_offset > 0:
                timestamps_to_prepend = self.processed_dataframe.index[slice_start_idx - zero_offset : slice_start_idx]
                values_to_add = self.processed_dataframe.loc[timestamps_to_prepend, signal_to_slice]
                padded_signal = np.insert(padded_signal, 0, values_to_add)


            slices.append(padded_signal)


        # Now everything is lined up, but that doesn't mean that it's all the same length. 

        # We've gotten all the baselines lined up, so now it's just a matter of setting the epoch lengths right. 
        # We'll do this by lopping off any extra from the end. 
        min_slice_length = min(s.size for s in slices)
        self.trial_data = np.empty([len(slices), min_slice_length])

        for i, s in enumerate(slices):
            self.trial_data[i] = np.delete(s, range(min_slice_length, s.size))


    def shuffle_align(self, n_iterations = 1000, baseline_time = 10, epoch_time=10, signal_to_slice='Z_Signal'):
        ''' 
            This function will randomly select a number of timepoints equivalent to the number of TTLs registered
            and apply align_to_ttls to those timepoints. It will compute and store the average trace generated by this procedure. 
            It will perform this n_iterations number of times. 

            param self:                 Attributes of signal_processing_object
            param n_iterations:         The number of iterations for which to produce a shuffled alignment
            param baseline_time:        The baseline length to pass to align_to_TTLs
            param epoch_time:           The epoch length to pass to align_to_TTLs
            param signal_to_slice:      The signal ('Z_Signal' or 'RawSignal') which align_to_TTLs is to align
            create self.shuffle_means:  An array in which each row contains the average trace of a set of shuffled data. 
        '''
        # Begin by ensuring that a record of the True TTLs is maintained

        try:
            # If true_ttls already exists, then shuffle_align has been run before.
            # Do not overwrite. Generate a variable that will be used later (n_ttls) and move on.
            n_ttls = self.true_ttls.size
        except AttributeError:
            # If true_ttls does not yet exist, create it. 
            n_ttls = self.ttl_starts.size
            self.true_ttls = self.ttl_starts.copy()

        # Determine the number of bins to compare sliced data against (this is prevent the selection of timestamps from too 
        #    close to either end of the trial)
        try:
            bins = self.trial_data.shape[1]
        except AttributeError as e:
            raise Exception("I'm not going to let you run these shuffles before generating trial_data. Why risk guessing the bin size? Run align_to_TTLs and then try again.") from e           

        # Now generate the shuffled data.
        self.shuffle_means = np.empty([n_iterations, bins]) # Array for storing output
        for i in range(n_iterations):

            # A sort of do/while loop here. Only exit the loop if a mean_signal is generated that will fit in self.shuffle_means 
            # Timestamps within baseline_time from the beginning of the session or epoch_time from the end will produce data with fewer than n_bins
            generate_ttls = True
            while generate_ttls: 
                # Assume that it will get it right right away and prepare to exit loop. 
                generate_ttls = False
                
                # Select the random timestamps and align the data.
                rand_ttls = random.sample(list(self.timestamps), n_ttls)
                self.ttl_starts = rand_ttls
                self.align_to_TTLs(baseline_time = baseline_time, epoch_time=epoch_time, signal_to_slice=signal_to_slice)

                # Take the average and check whether it looks good.
                mean_signal = np.nanmean(self.trial_data, axis=0)
                if mean_signal.size < bins:
                    # Try the procedure again if the data aren't the right size
                    generate_ttls = True
            
            # Ensure that when we get to the next iteration of the larger for loop, we'll generate new TTLs
            generate_ttls = True
            
            # Store the data
            normed = calc_robust_z(mean_signal) 
            self.shuffle_means[i] = normed[:bins]

                
        # Before exiting the function, re-align the data to the true TTLs. 
        self.ttl_starts = self.true_ttls.copy()
        self.align_to_TTLs(baseline_time = baseline_time, epoch_time=epoch_time, signal_to_slice=signal_to_slice)